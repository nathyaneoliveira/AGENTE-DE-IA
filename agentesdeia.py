# -*- coding: utf-8 -*-
"""agentesdeIA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qh8S4aTpTXN77pxWkkKy273H6bLZ1VeR

#  IMERSÃO DEV AGENTES DE IA (ALURA E GOOGLE)

USAR IA PELO AI STUDIO
LINK -> https://aistudio.google.com/app/prompts/new_chat

FAZER UMA TRIAGEM DE COMO VAI USAR O MODELO -> https://www.tldraw.com/

INSTALAR AS BIBLIOTECAS
"""

#LER PDF, QUEBRA O PDF EM PEDAÇOS E CONECTA A BIBLIOTECA
!pip install -q --upgrade langchain_community faiss-cpu langchain-text-splitters pymupdf

#Esse comando atualiza o LangChain e instala as bibliotecas que permitem usar o Gemini (Google Generative AI) tanto direto quanto integrado ao LangChain.
!pip install --upgrade langchain langchain-google-genai google-generativeai

#ajuda a criar fluxos de agentes e cadeias de raciocínio como se fossem grafos.
!pip install -q --upgrade langgraph

"""-> IMPORTAÇÃO DA API KEY
-> CHAMANDO MODELO
"""

from google.colab import userdata
from langchain_google_genai import ChatGoogleGenerativeAI

GOOGLE_API_KEY = userdata.get('gemini') #ATIVANDO CHAVE DE ACESSO A IA

"""PREPARAR O MODELO QUE VAI ULTILIZAR

MODELO ULTILIZADO -> GEMINI
"""

llm = ChatGoogleGenerativeAI(
    model = 'gemini-2.5-flash',
    temperatura = 0,
    api_key= GOOGLE_API_KEY
)

resp_test = llm.invoke ('Quem é você? Seja criativo')
print(resp_test.content)

"""FAZENDO O PROMPT DE COMANDO, DE COMO ELE VAI FUNCIONAR"""

TRIAGEM_PROMPT = (
    "Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. "
    "Dada a mensagem do usuário, retorne SOMENTE um JSON com:\n"
    "{\n"
    '  "decisao": "AUTO_RESOLVER" | "PEDIR_INFO" | "ABRIR_CHAMADO",\n'
    '  "urgencia": "BAIXA" | "MEDIA" | "ALTA",\n'
    '  "campos_faltantes": ["..."]\n'
    "}\n"
    "Regras:\n"
    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: "Posso reembolsar a internet do meu home office?", "Como funciona a política de alimentação em viagens?").\n'
    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: "Preciso de ajuda com uma política", "Tenho uma dúvida geral").\n'
    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: "Quero exceção para trabalhar 5 dias remoto.", "Solicito liberação para anexos externos.", "Por favor, abra um chamado para o RH.").'
    "Analise a mensagem e decida a ação mais apropriada."
)

"""-> CHAMANDO AS BIBLIOTECAS PYDANTIC E BASEMODEL

-> CHAMANDO UM DICIONÁRIO DE DADOS -> TYPING

    LITERAL - RESOLVER, PEDIR INFORMAÇÃO E ABRIR CHAMADO
    LIST (LISTA) - DECISÃO, URGÊNCIA E SE TEM CAMPOS FALTANTES
    DICT - É O DICIONÁRIO

"""

from pydantic import BaseModel, Field
from typing import Literal, List, Dict

class TriagemOut(BaseModel):
    decisao: Literal["AUTO_RESOLVER", "PEDIR_INFO", "ABRIR_CHAMADO"]
    urgencia: Literal["BAIXA", "MEDIA", "ALTA"]
    campos_faltantes: List[str] = Field(default_factory=list)

"""AGENTE DE IA"""

llm_triagem = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.0,
    api_key=GOOGLE_API_KEY
)

"""IMPORTAÇÕES PARA ESTRUTURAR AS MENSAGENS ->
DIFERENCIAR O QUE É DO SISTEMA E O QUE É DO USUÁRIO
"""

from langchain_core.messages import SystemMessage, HumanMessage

triagem_chain = llm_triagem.with_structured_output(TriagemOut)

def triagem(mensagem: str) -> Dict:
    saida: TriagemOut = triagem_chain.invoke([
        SystemMessage(content=TRIAGEM_PROMPT),
        HumanMessage(content=mensagem)
    ])

    return saida.model_dump()

"""TESTE DAS PERGUNTAS"""

testes = ["Posso reembolsar a internet?",
          "Quero mais 5 dias de trabalho remoto. Como faço?",
          "Posso reembolsar cursos ou treinamentos da Alura?",
          "Quantas capivaras tem no Rio Pinheiros?"]

"""SAÍDA

"""

for msg_teste in testes:
    print(f"Pergunta: {msg_teste}\n -> Resposta: {triagem(msg_teste)}\n")

"""-> CARREGAR E LER DOCUMENTOS"""

from pathlib import Path
from langchain_community.document_loaders import PyMuPDFLoader

docs = []

for n in Path("/content/").glob("*.pdf"): #CAMINHO DA PASTA DE DOCUMENTOS
    try:
        loader = PyMuPDFLoader(str(n)) #CARREGA O CAMINHO PyMuPDFLoader
        docs.extend(loader.load())
        print(f"Carregado com sucesso arquivo {n.name}")
    except Exception as e:
        print(f"Erro ao carregar arquivo {n.name}: {e}")

print(f"Total de documentos carregados: {len(docs)}")

"""DEFINIR O TAMANHO DO DOCUMENTO"""

from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30) #DIVIDE O TEXTO EM PEDAÇOS

chunks = splitter.split_documents(docs)

for chunk in chunks: #SEPARADOR DO TEXTO DE DOCUMENTOS PRINTADOS
    print(chunk)
    print("------------------------------------")

from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/gemini-embedding-001", #REALIZAR BUSCAS DENTRO DO ESPAÇO
    google_api_key=GOOGLE_API_KEY
)

"""CALCULAR SIMILARIDADE"""

from langchain_community.vectorstores import FAISS

vectorstore = FAISS.from_documents(chunks, embeddings) #VAI CONVERTER DE CHUNKS PARA VECTORS

retriever = vectorstore.as_retriever(search_type="similarity_score_threshold",
                                     search_kwargs={"score_threshold":0.3, "k": 4})

"""("system", ...) → define o papel do modelo (um assistente de RH/IT da empresa).

("human", ...) → define como a pergunta do usuário e os documentos recuperados serão passados.

{input} → é a pergunta do usuário.

{context} → são os documentos recuperados (ex.: políticas da empresa).
"""

from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain

prompt_rag = ChatPromptTemplate.from_messages([
    ("system",
     "Você é um Assistente de Políticas Internas (RH/IT) da empresa Carraro Desenvolvimento. "
     "Responda SOMENTE com base no contexto fornecido. "
     "Se não houver base suficiente, responda apenas 'Não sei'."),

    ("human", "Pergunta: {input}\n\nContexto:\n{context}")
])

document_chain = create_stuff_documents_chain(llm_triagem, prompt_rag)

"""Recebe docs_rel → lista de documentos recuperados pelo retriever (FAISS, Chroma, etc.).

source → nome do arquivo (ex.: politicas.pdf).

page → número da página (se veio nos metadados).

Usa um set seen para evitar duplicatas (mesmo arquivo/página).

Para cada documento, monta um dicionário com:

documento → nome do arquivo.

pagina → página do PDF (+1, porque em metadados costuma começar em 0).

trecho → o pedacinho do texto extraído pela extrair_trecho.

Retorna no máximo 3 citações.
"""

# Formatadores
import re, pathlib

def _clean_text(s: str) -> str:   #Remove espaços em excesso, quebras de linha e tabs e garante que o texto fique "limpo" e padronizado numa única linha.
    return re.sub(r"\s+", " ", s or "").strip()

def extrair_trecho(texto: str, query: str, janela: int = 240) -> str:  #LIMPA, EXTRAI E RECORTA O TEXTO
    txt = _clean_text(texto)
    termos = [t.lower() for t in re.findall(r"\w+", query or "") if len(t) >= 4]
    pos = -1
    for t in termos:
        pos = txt.lower().find(t)
        if pos != -1: break
    if pos == -1: pos = 0
    ini, fim = max(0, pos - janela//2), min(len(txt), pos + janela//2)
    return txt[ini:fim]

def formatar_citacoes(docs_rel: List, query: str) -> List[Dict]:
    cites, seen = [], set()
    for d in docs_rel:
        src = pathlib.Path(d.metadata.get("source","")).name
        page = int(d.metadata.get("page", 0)) + 1
        key = (src, page)
        if key in seen:
            continue
        seen.add(key)
        cites.append({"documento": src, "pagina": page, "trecho": extrair_trecho(d.page_content, query)})
    return cites[:3]

"""Modelo tipado de dicionário que organiza todo o fluxo de raciocínio e resposta do agente."""

from typing import TypedDict, Optional

class AgentState(TypedDict, total = False):
    pergunta: str #armazena a pergunta feita pelo usuário.
    triagem: dict #guarda informações de classificação/triagem da pergunta.
    resposta: Optional[str] #resposta do agente (pode ser None se ainda não existir).
    citacoes: List[dict]  #referências/documentos usados na resposta.
    rag_sucesso: bool #indica se a busca por contexto (RAG) deu certo.
    acao_final: str #qual foi a ação/decisão final do agente.

"""Define um nó de auto-resolução para o agente"""

def node_triagem(state: AgentState) -> AgentState:
    print("Executando nó de triagem...")
    return {"triagem": triagem(state["pergunta"])}

def node_auto_resolver(state: AgentState) -> AgentState: #CRIA UM DICIONÁRIO
    print("Executando nó de auto_resolver...")
    resposta_rag = perguntar_politica_RAG(state["pergunta"])

    update: AgentState = {
        "resposta": resposta_rag["answer"],
        "citacoes": resposta_rag.get("citacoes", []),
        "rag_sucesso": resposta_rag["contexto_encontrado"],
    }

    if resposta_rag["contexto_encontrado"]:
        update["acao_final"] = "AUTO_RESOLVER"

    return update

"""Define um nó de “pedir informações” para o agente, que é usado quando a triagem detecta que faltam dados para responder à pergunta."""

def node_pedir_info(state: AgentState) -> AgentState:
    print("Executando nó de pedir_info...")
    faltantes = state["triagem"].get("campos_faltantes", []) #Verifica os campos faltantes na triagem
    if faltantes:
        detalhe = ",".join(faltantes)
    else:
        detalhe = "Tema e contexto específico"

    return {
        "resposta": f"Para avançar, preciso que detalhe: {detalhe}",
        "citacoes": [],
        "acao_final": "PEDIR_INFO"
    }

"""define o nó de “abrir chamado” no fluxo do agente. Ele é acionado quando a triagem indica que a solicitação precisa ser escalada para atendimento humano ou sistema de chamados."""

def node_abrir_chamado(state: AgentState) -> AgentState:
    print("Executando nó de abrir_chamado...")
    triagem = state["triagem"]

    return {
        "resposta": f"Abrindo chamado com urgência {triagem['urgencia']}. Descrição: {state['pergunta'][:140]}",
        "citacoes": [],
        "acao_final": "ABRIR_CHAMADO" #Para indicar que o agente abriu o chamado
    }

"""Define a função de decisão após a triagem do agente, ou seja, ele escolhe qual nó do fluxo deve ser executado com base no resultado da triagem."""

KEYWORDS_ABRIR_TICKET = ["aprovação", "exceção", "liberação", "abrir ticket", "abrir chamado", "acesso especial"]

def decidir_pos_triagem(state: AgentState) -> str:
    print("Decidindo após a triagem...")
    decisao = state["triagem"]["decisao"]

    if decisao == "AUTO_RESOLVER": return "auto"
    if decisao == "PEDIR_INFO": return "info"
    if decisao == "ABRIR_CHAMADO": return "chamado"

def decidir_pos_auto_resolver(state: AgentState) -> str:
    print("Decidindo após o auto_resolver...")

    if state.get("rag_sucesso"):  #Verifica se o RAG teve sucesso:
        print("Rag com sucesso, finalizando o fluxo.")
        return "ok"

    state_da_pergunta = (state["pergunta"] or "").lower() #Converte a pergunta para minúsculas:

    if any(k in state_da_pergunta for k in KEYWORDS_ABRIR_TICKET):  #Verifica se há palavras-chave para abrir ticket:
        print("Rag falhou, mas foram encontradas keywords de abertura de ticket. Abrindo...")
        return "chamado"

    print("Rag falhou, sem keywords, vou pedir mais informações...")
    return "info"

from langgraph.graph import StateGraph, START, END

workflow = StateGraph(AgentState) #Criação do grafo do fluxo

#Adicionando os nós do fluxo
workflow.add_node("triagem", node_triagem)
workflow.add_node("auto_resolver", node_auto_resolver)
workflow.add_node("pedir_info", node_pedir_info)
workflow.add_node("abrir_chamado", node_abrir_chamado)

workflow.add_edge(START, "triagem") #Definindo as conexões iniciais

#Definindo decisões condicionais
workflow.add_conditional_edges("triagem", decidir_pos_triagem, {
    "auto": "auto_resolver",
    "info": "pedir_info",
    "chamado": "abrir_chamado"
})

workflow.add_conditional_edges("auto_resolver", decidir_pos_auto_resolver, {
    "info": "pedir_info",
    "chamado": "abrir_chamado",
    "ok": END
})
#Definindo nós finais
workflow.add_edge("pedir_info", END)
workflow.add_edge("abrir_chamado", END)

grafo = workflow.compile()  #Compilando o grafo

#Gerar uma visualização do seu fluxo de estados (grafo) como imagem usando Mermaid via IPython.
from IPython.display import display, Image

graph_bytes = grafo.get_graph().draw_mermaid_png()
display(Image(graph_bytes))  #Mostra a imagem gerada

testes = ["Posso reembolsar a internet?",
          "Quero mais 5 dias de trabalho remoto. Como faço?",
          "Posso reembolsar cursos ou treinamentos da Alura?",
          "É possível reembolsar certificações do Google Cloud?",
          "Posso obter o Google Gemini de graça?",
          "Qual é a palavra-chave da aula de hoje?",
          "Quantas capivaras tem no Rio Pinheiros?"]

"""Testando o seu workflow de atendimento automático"""

for msg_test in testes: #Loop pelas perguntas de teste
    resposta_final = grafo.invoke({"pergunta": msg_test}) #Invoca o grafo com a pergunta

#Mostra informações detalhadas
    triag = resposta_final.get("triagem", {})
    print(f"PERGUNTA: {msg_test}")
    print(f"DECISÃO: {triag.get('decisao')} | URGÊNCIA: {triag.get('urgencia')} | AÇÃO FINAL: {resposta_final.get('acao_final')}")
    print(f"RESPOSTA: {resposta_final.get('resposta')}")
#Mostra citações, se houver
    if resposta_final.get("citacoes"):
        print("CITAÇÕES:")
        for citacao in resposta_final.get("citacoes"):
            print(f" - Documento: {citacao['documento']}, Página: {citacao['pagina']}")
            print(f"   Trecho: {citacao['trecho']}")

    print("------------------------------------")